# Lecture 10
## Turing and Machine Intelligence
Can machines think? This has developed into the study of Artificial Intelligence.

* **Weak AI**: computing processes provide useful explanatory models for studying human cognition.

* **Strong AI**: Suitably designed computing systems may actually think.

Instead of *can machines think,* Turing attempts to determine whether machines can do well in the *Imitation Game.*

### The Imitation Game
* Three players: A: a man, B: a woman, and C: an interrogator.
* A and B are hidden from C. Interrogation is via a computer.

### Roles:
* C's role: Determine which is the man and which is the woman. 
* A's role: Try to fool C into making the wrong identification.
* B's role: Try to help C make the right identification.

This game is important to AI if one of the participants is replaced with a computer.

C must determine which participant is the human and which is the computer.

If A is the computer, A is trying to make C think it is a human and B, the human, is trying to prove to C that he is actually the human.

This is the **Turing Test.** This is a test for intelligence and thinking.

A computer able to fool C should be counted as intelligent and considered to be a machine that thinks.

A machine that passes the test behaves like a human and is intelligent just like a human.

Turing believes that machines like this do not exist because of the limitations of his time, not any fundamental or conceptual divide between computers and humans.


Objections to this is the argument that machines are not conscious. Turing suggests that even if computers always will lack consciousness, why does this mean that computers cannot think?

Couldn't the processes that machines undergo in the Turing Test be considered thinking?

Why should processes be conscious to be thought processes?

Additionally, computers only do what they are programmed to do. If they are intelligent, it is the intelligence of the party that programmed it, not the machine itself.

What if all that humans do is a program? We could just be running a long, complicated program ourselves.

Perhaps a certain kind of programming could lead to the a computer that resembles the mind of a child. Updating and modifying itself as new data is presented to it.

Under this assumption, there is no opposition to developing intelligent computers. There is nothing irreplaceable in a human that could prevent computers from being considered intelligent in the future.

### John Searle's Critique
Searle considers human beings as machines, biological machines that can think. Their hardware, and physiology produces thought. Human beings have brains.

Brains have causal powers such as the causation of mental states. Brains cause mental states, humans have brain, therefore humans can have mental thought.

But computers have programs, would a computer count as thinking if it ran a program that accurately modeled human thought processes?

Searle answers **No.** Searle believes that even if you had a machine that could pass the Turing Test, it still does not thinking and that to consider it analogous to human thought as wrong.

Searle believes that computers lack something important.

Computers run programs. Searle investigates what this means. For Searle, to run a computer program is a matter of formally manipulating symbols according to syntactic rules.

Computers respond in a definite way to the symbolic material that goes into its program and it responds only to the formal or syntactic elements of the symbols that constitute its program.

Computers only respond to the definite shape of the symbols and the manner they are put in order, the grammar of the symbolic material.

Searle's conclusion:

*Mere syntactic symbol manipulation as computers do, can never amount to thought.*

But why doesn't it amount to thought?

### The Chinese Room Experiment
Imagine oneself as a computer that runs a program for simulating written communication in Chinese.

You're behind a door in a locked room and being given sequences of Chinese symbols. Having no knowledge of what they mean, but you have a manual that tells you that when certain sequences come through the slot, you should output certain symbols in return.

Like using a language translator in two languages you don't know. You get things back and put things in but don't understand anything.

You apply the rules to take the sequences and return the appropriate response without knowing what's happening.

Unknown to the person, the sequences are meaningful sequences in Chinese, as are the responses.

You are passing the Turing test because you are responding in the appropriate way. You would pass the Turing test by simply returning the appropriate response. But do you really know what's going on?

**NO!!!**

You are doing what a computer does but it is clear that you are not thinking. You are just doing I/O without any thought, just wrote application of rules.

People who actually speak Chinese would respond in their own way. Not through a rule. You are not thinking any of the thoughts that a native speaker would have.

You are doing what a computer does, not what a person does. You have the syntax and the formal means of returning the correct sentences without any understanding.

This is the lack that computers have. Even if they passed the Turing test, it does not constitute actual thought.

Running a program is insufficient for thought. Syntactic symbol manipulation is not considered to be thought.

## What's Missing?
Human thought is not just syntactic processing and symbol manipulation. Searle thinks that what humans do in addition is that human thought has a semantic or representational dimension.

Humans understand what the symbols mean, the semantics of the symbols.

When someone says 'horse' they are referring to an animal. Whereas a computer does not know what that word means.

Human thought has representational content and it is critical to cognition. Thoughts are about something.

Computers do not grasp representational content. Searle's response is that this ability of humans is a function of our nature as biological beings. We have mental states and can undergo thought processes.

Brains cause mental states. We can think because we have a brain biology that can produce mental states. The hardware architecture that computers have outfits them well for symbolic manipulation, but it is a hardware that can not have mental states.

To Searle, the difference is that brain physiology has causal capacity while hardware equipment has only syntactic and symbol manipulation.

It is just a fact of our biology that we have this causal power. It is not a necessary truth that only humans can have this power, but just not the sort of hardware our computers run on.

But this alien physiology would need to have equivalent causal power. It would have to go beyond computers and be capable of thinking and realizing thoughts.

They would have to have distinctive causal powers.

*Brains cause minds.*

It is a distinctive causal power of our biology that we can have mental states and we don't know how to duplicate that yet. Our computers only have syntactic symbol manipulation and this never leads to semantic content.

Computers operate on formal and syntactic features of their input. There is no semantic component.

Minds, things that think, do have semantic, representational content.

No computer is sufficient to give a system a mind. If you thought you could adequately capture human thought by running a process then according to Searle, you are leaving out the manner in which real minds have semantic content.

But is it any less mysterious for computers as there is for humans? How do humans achieve representational, semantic thinking?

Humans can think about the symbols on a meta level, to refer beyond themselves, how is this achieved?

### Fodor's Criticism of Searle
The best candidate for explaining how mental states in human beings carry representational content is that that rep. cont. can be thought of in functional terms.

Take a mental state, like the belief that John is tall. A functionalist, Fodor, argues that the belief's representational content is determined by their functional and causal relationships.

Let's say that the belief is caused by visual and causal interaction with John. It is in response to this stimulus that the semantic content is developed.

The belief is caused by John, and the property that constitutes his tallness and this gives the belief its meaning.

The functionalist considers the effects this mental state has. The belief of this kind has certain results. This could result in the functional action of verbalizing that 'John is tall'.

This kind of output specifies the sort of representational content that the state carries. The mental state has a cause and a result. 

A functionalist believes that mental states interact, so a belief like John is tall will lead to other beliefs that relate to it in logical terms.

A belief that John is tall means that someone else is tall or someone else is shorter than John.

The functional roles: cause, effect, interactions, can form the representational content that humans have.

If this is the right way to get representational content for humans, can computers get this kind of representational content?

Are there causes in a computer's state, effects and the interactions that result?

Could no computer qualify as this definition of representational thought, mental though that could be called thinking?

A computer with eyes, sound detectors, light detectors, and other input devices would be able to take in information like the human senses.

A machine with so much input could have a state like 'John's tallness' and be able to deliver outputs, as well as interact with other states.

The robotic computer could have states equivalent to the mental states people do such as 'John is tall'. There is no obstacle to saying that computers could have internal states similar to humans in their representation of the external world.

The computer could have representational content in the same sense as humans do.

But then what does representation involve?

Serle's main complaint about computers is that they cannot realize semantic content because it requires a brain.

No computing machine can have these causal powers.

It cannot be the same as human representational content because the causal powers associated with the computer machinery will be very different than from human brains.

### Zenon Pylyshyn's Critique
Agrees with Fodor on the merits of functional accounts and intentionality or representation.

> A computing machine that realizes the same functional relationships as a human thinker would be representing the same ways.

What about Searle's claims concerning the causal powers of brains?

Is it not a kind of chauvinism to believe that brains are so important? What if the cells in the brain were replaced with integrated circuit chips with equivalent input/output functions?

When does a brain become a chip? Or is it still a brain? It is functionally doing the same thing.

If the right kinds of states are created with the same sort of relationships, does the medium matter?

The functionalists, Fodor and Pylyshyn would say no.

Representation is not uniquely achievable by human or humanesque physiology.

If replacing the brain with a machine, we do not lose anything, so reverse engineering, is that machine not a thinking machine?

Should a suitably outfitted computer and with the right set of states, causes, effects and interactions be able to think?

It could still be argued that the machine would lack consciousness, a critical component of human mental life.

But should the possession of consciousness be a criterion for thinking?

A lot of people do not believe consciousness is required for thinking. Even if computing machines lack consciousness, they can still have representational internal states. They can have thoughts that have content like humans do.

Pylyshyn's Main Conclusion:

People arguably don't know that much about the right account of human representation. Until we know more about how representation works in the human case, we shouldn't rule out the idea that machine states could be representational.

We don't know how human minds get the representational content that they do and it is rash to assume you need a something like a human brain like Searle has said.







